{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano' # Using theano as backend instead of tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = 'ILDC_multi/ILDC_multi.csv'\n",
    "\n",
    "# path to transformer generated chunk embeddings eg. XLNet etc.\n",
    "path_to_transformer_chunk_embeddings_train = 'emb/XLNet_train.npy'\n",
    "path_to_transformer_chunk_embeddings_dev = 'emb/XLNet_dev.npy'\n",
    "path_to_transformer_chunk_embeddings_test = 'emb/XLNet_test.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(path_to_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train0 = load(path_to_transformer_chunk_embeddings_train, allow_pickle = True)\n",
    "x_dev0 = load(path_to_transformer_chunk_embeddings_dev, allow_pickle= True)\n",
    "x_test0 = load(path_to_transformer_chunk_embeddings_test, allow_pickle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the corresponding label for each case in dataset\n",
    "dev = dataset.loc[dataset['split'] == 'dev']\n",
    "train = dataset.loc[dataset['split'] == 'train']\n",
    "test = dataset.loc[dataset['split'] == 'test']\n",
    "\n",
    "y_train0 = []\n",
    "for i in range(train.shape[0]):\n",
    "    y_train0.append(train.loc[i,'label'])\n",
    "\n",
    "y_dev0 = []\n",
    "for i in range(dev.shape[0]):\n",
    "    y_dev0.append(dev.loc[i+32305,'label'])\n",
    "\n",
    "y_test0 = []\n",
    "for i in range(test.shape[0]):\n",
    "    y_test0.append(test.loc[i+33299,'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5082\n",
      "(994, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32287"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_train0))\n",
    "print(dev.shape)\n",
    "1008*32+31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the maximum sequnce length and embedding dimension\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "EMBEDDING_DIM = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the vectors to maximum sequence length\n",
    "for i in range(x_train0.shape[0]):\n",
    "    padding_vector = np.zeros((MAX_SEQUENCE_LENGTH - x_train0[i].shape[0], EMBEDDING_DIM))\n",
    "    x_train0[i] = np.concatenate((x_train0[i],padding_vector), axis = 0)\n",
    "    \n",
    "for i in range(x_dev0.shape[0]):\n",
    "    padding_vector = np.zeros((MAX_SEQUENCE_LENGTH - x_dev0[i].shape[0], EMBEDDING_DIM))\n",
    "    x_dev0[i] = np.concatenate((x_dev0[i],padding_vector),axis = 0)\n",
    "    \n",
    "for i in range(x_test0.shape[0]):\n",
    "    padding_vector = np.zeros((MAX_SEQUENCE_LENGTH - x_test0[i].shape[0], EMBEDDING_DIM))\n",
    "    x_test0[i] = np.concatenate((x_test0[i],padding_vector), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            (None, 25, 768)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 24, 256)           393472    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 12, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 11, 128)           65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 4, 128)            32896     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 512,705\n",
      "Trainable params: 512,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using Input layer to convert into required tensor shape\n",
    "text_input = Input(shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM), dtype='float32', name='text')\n",
    "# Using 3 Conv1D layers followed by max pooling layers\n",
    "l_cov1= Conv1D(256, 2, activation='relu')(text_input)\n",
    "l_pool1 = MaxPooling1D(2)(l_cov1)\n",
    "l_cov2 = Conv1D(128, 2, activation='relu')(l_pool1)\n",
    "l_pool2 = MaxPooling1D(2)(l_cov2)\n",
    "l_cov3 = Conv1D(128, 2, activation='relu')(l_pool2)\n",
    "l_pool3 = MaxPooling1D(4)(l_cov3)  # global max pooling\n",
    "# Using the flatten layer to convert into 1D tensor\n",
    "l_flat = Flatten()(l_pool3)\n",
    "# passing the output embeddings through 2 dense layers\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "l_dense1 = Dense(32, activation='relu')(l_dense)\n",
    "# Using sigmoid classifier\n",
    "preds = Dense(1, activation='sigmoid')(l_dense1)\n",
    "\n",
    "model = Model(text_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = len(x_train0)\n",
    "batch_size = 32\n",
    "batches_per_epoch =  int(num_sequences/batch_size)\n",
    "num_features= 768\n",
    "def train_generator(): # function to generate batches of corresponding batch size\n",
    "    x_list= x_train0\n",
    "    y_list =  y_train0\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(x_train0[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences_val = len(x_dev0)\n",
    "batch_size_val = 32\n",
    "batches_per_epoch_val = int(num_sequences_val/batch_size_val)\n",
    "num_features= 768\n",
    "def val_generator(): # function to generate batches of corresponding batch size\n",
    "    x_list= x_dev0\n",
    "    y_list =  y_dev0\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(x_dev0[:(b + 1) * batch_size_val][-batch_size_val:], key=len))\n",
    "            x_train = np.full((batch_size_val, timesteps, num_features), 0)\n",
    "            y_train = np.zeros((batch_size_val,  1))\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features= 768\n",
    "def test_generator(): # function to generate batches of corresponding batch size\n",
    "    x_list= x_test0\n",
    "    y_list =  y_test0\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_test):\n",
    "            if(b == batches_per_epoch_test-1): # An extra if else statement just to manage the last batch as it's size might not be equal to batch size \n",
    "                longest_index = num_sequences_test - 1\n",
    "                timesteps = len(max(x_test0[:longest_index + 1][-batch_size_test:], key=len))\n",
    "                x_train = np.full((longest_index - b*batch_size_test, timesteps, num_features), -99.)\n",
    "                y_train = np.zeros((longest_index - b*batch_size_test,  1))\n",
    "                for i in range(longest_index - b*batch_size_test):\n",
    "                    li = b * batch_size_test + i\n",
    "                    x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                    y_train[i] = y_list[li]\n",
    "            else:\n",
    "                longest_index = (b + 1) * batch_size_test - 1\n",
    "                timesteps = len(max(x_test0[:(b + 1) * batch_size_test][-batch_size_test:], key=len))\n",
    "                x_train = np.full((batch_size_test, timesteps, num_features), -99.)\n",
    "                y_train = np.zeros((batch_size_test,  1))\n",
    "                for i in range(batch_size_test):\n",
    "                    li = b * batch_size_test + i\n",
    "                    x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                    y_train[i] = y_list[li]\n",
    "            yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1009/1009 [==============================] - 47s 47ms/step - loss: 0.4631 - acc: 0.7795 - val_loss: 0.5781 - val_acc: 0.7107\n",
      "Epoch 2/10\n",
      "1009/1009 [==============================] - 55s 55ms/step - loss: 0.4445 - acc: 0.7878 - val_loss: 0.5788 - val_acc: 0.6905\n",
      "Epoch 3/10\n",
      "1009/1009 [==============================] - 57s 56ms/step - loss: 0.4332 - acc: 0.7955 - val_loss: 0.5763 - val_acc: 0.7016\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "Epoch 4/10\n",
      "1009/1009 [==============================] - 58s 57ms/step - loss: 0.4204 - acc: 0.8007 - val_loss: 0.5672 - val_acc: 0.6996\n",
      "Epoch 5/10\n",
      "1009/1009 [==============================] - 58s 58ms/step - loss: 0.4076 - acc: 0.8085 - val_loss: 0.5749 - val_acc: 0.6895\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "Epoch 6/10\n",
      "1009/1009 [==============================] - 60s 59ms/step - loss: 0.3887 - acc: 0.8201 - val_loss: 0.5798 - val_acc: 0.6815\n",
      "Epoch 7/10\n",
      "1009/1009 [==============================] - 60s 60ms/step - loss: 0.3702 - acc: 0.8294 - val_loss: 0.5576 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "Epoch 8/10\n",
      "1009/1009 [==============================] - 61s 60ms/step - loss: 0.3514 - acc: 0.8383 - val_loss: 0.5482 - val_acc: 0.7087\n",
      "Epoch 9/10\n",
      "1009/1009 [==============================] - 61s 61ms/step - loss: 0.3352 - acc: 0.8462 - val_loss: 0.5274 - val_acc: 0.6774\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0008145062311086804.\n",
      "Epoch 10/10\n",
      "1009/1009 [==============================] - 62s 62ms/step - loss: 0.3200 - acc: 0.8534 - val_loss: 0.5817 - val_acc: 0.6815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa793b70390>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the callback and training the model\n",
    "call_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=2, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)\n",
    "\n",
    "model.fit_generator(train_generator(), steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_generator(), validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5710874199867249, 0.7077836394309998]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences_test = len(x_test0)\n",
    "batch_size_test = 32\n",
    "batches_per_epoch_test = int(num_sequences_test/batch_size_test) + 1\n",
    "num_features= 768\n",
    "# evaluating on the test data\n",
    "model.evaluate_generator(test_generator(), steps= batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function which calculates various metrics such as micro and macro precision, accuracy and f1\n",
    "def metrics_calculator(preds, test_labels):\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    TP = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[i][j]\n",
    "\n",
    "        FN.append(summ)\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[j][i]\n",
    "\n",
    "        FP.append(summ)\n",
    "    for i in range(0,2):\n",
    "        TP.append(cm[i][i])\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(0,2):\n",
    "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
    "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
    "\n",
    "    macro_precision = sum(precision)/2\n",
    "    macro_recall = sum(recall)/2\n",
    "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
    "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
    "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
    "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
    "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7262697561976874, 0.7072125383992829, 0.7166144706204457, 0.7077836411609498, 0.7077836411609498, 0.7077836411609497)\n"
     ]
    }
   ],
   "source": [
    "# getting the predicted labels on the test data\n",
    "preds = model.predict_generator(test_generator(), steps= batches_per_epoch_test)\n",
    "y_pred = preds > 0.5\n",
    "\n",
    "# Calculating all metrics on test data predicted label\n",
    "print(metrics_calculator(y_pred, y_test0[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6873489121676067, 0.6814516129032258, 0.6843875586614856, 0.6814516129032258, 0.6814516129032258, 0.6814516129032258)\n"
     ]
    }
   ],
   "source": [
    "# getting the predicted labels on the dev data\n",
    "preds = model.predict_generator(val_generator(), steps= batches_per_epoch_val)\n",
    "y_pred_dev = preds > 0.5\n",
    "\n",
    "# Calculating all metrics on dev data predicted label\n",
    "print(metrics_calculator(y_pred_dev, y_dev0[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
