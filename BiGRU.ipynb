{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import LSTM, Dense, Masking, GRU\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dense, Input, concatenate, Layer, Lambda, Dropout, Activation\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback, TensorBoard\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "np.random.seed(1337)# setting the random seed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = \"ILDC_single/ILDC_single.csv\" # path to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(path_dataset) # loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_transformer_chunk_embeddings_train = 'emb/XLNet_single_train.npy'\n",
    "path_to_transformer_chunk_embeddings_dev = 'emb/XLNet_single_dev.npy'\n",
    "path_to_transformer_chunk_embeddings_test = 'emb/XLNet_single_test.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train0 = load(path_to_transformer_chunk_embeddings_train, allow_pickle = True)\n",
    "x_dev0 = load(path_to_transformer_chunk_embeddings_dev, allow_pickle= True)\n",
    "x_test0 = load(path_to_transformer_chunk_embeddings_test, allow_pickle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train0=x_train0[:5082,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5082,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = dataset.loc[dataset['split'] == 'dev'] \n",
    "train = dataset.loc[dataset['split'] == 'train'] \n",
    "test = dataset.loc[dataset['split'] == 'test'] \n",
    "\n",
    "y_train0 = []\n",
    "for i in range(train.shape[0]):\n",
    "    y_train0.append(train.loc[i,'label'])  \n",
    "    \n",
    "y_dev0 = []\n",
    "for i in range(dev.shape[0]):\n",
    "    y_dev0.append(dev.loc[i+6599,'label'])\n",
    "\n",
    "y_test0 = []\n",
    "for i in range(test.shape[0]):\n",
    "    y_test0.append(test.loc[i+5082,'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, None, 768)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 200)         521400    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 200)               180600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 708,061\n",
      "Trainable params: 708,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "# Input layer to convert into required tensor shape\n",
    "text_input = Input(shape=(None,768,), dtype='float32', name='text')\n",
    "# Masking layer to mask the padded values\n",
    "l_mask = layers.Masking(mask_value=-99.)(text_input)\n",
    "# After masking we encoded the vector using 2 bidirectional GRU's\n",
    "encoded_text = layers.Bidirectional(GRU(100,return_sequences=True))(l_mask)\n",
    "encoded_text1 = layers.Bidirectional(GRU(100,))(encoded_text)\n",
    "# Added a dense layer after encoding\n",
    "out_dense = layers.Dense(30, activation='relu')(encoded_text1)\n",
    "#out_dense2 = layers.Dense(16, activation='relu')(out_dense)\n",
    "# And we add a sigmoid classifier on top\n",
    "out = layers.Dense(1, activation='sigmoid')(out_dense)\n",
    "# At model instantiation, we specify the input and the output:\n",
    "model = Model(text_input, out)\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = len(x_train0)\n",
    "batch_size = 32 \n",
    "batches_per_epoch =  int(num_sequences/batch_size)\n",
    "num_features= 768\n",
    "def train_generator(): # function to generate batches of corresponding batch size\n",
    "    x_list= x_train0\n",
    "    y_list =  y_train0\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch):\n",
    "            longest_index = (b + 1) * batch_size - 1\n",
    "            timesteps = len(max(x_train0[:(b + 1) * batch_size][-batch_size:], key=len))\n",
    "            x_train = np.full((batch_size, timesteps, num_features), -99.)\n",
    "            y_train = np.zeros((batch_size,  1))\n",
    "            # padding the vectors with respect to the maximum sequence of each batch and not the whole training data\n",
    "            for i in range(batch_size):\n",
    "                li = b * batch_size + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences_val = len(x_dev0)\n",
    "batch_size_val = 32\n",
    "batches_per_epoch_val = int(num_sequences_val/batch_size_val)\n",
    "num_features= 768\n",
    "def val_generator():# Similar function to generate validation batches\n",
    "    x_list= x_dev0\n",
    "    y_list =  y_dev0\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_val):\n",
    "            longest_index = (b + 1) * batch_size_val - 1\n",
    "            timesteps = len(max(x_dev0[:(b + 1) * batch_size_val][-batch_size_val:], key=len))\n",
    "            x_train = np.full((batch_size_val, timesteps, num_features), 0)\n",
    "            y_train = np.zeros((batch_size_val,  1))\n",
    "            # padding the vectors with respect to the maximum sequence of each batch and not the whole validation data\n",
    "            for i in range(batch_size_val):\n",
    "                li = b * batch_size_val + i\n",
    "                x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                y_train[i] = y_list[li]\n",
    "            yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "158/158 [==============================] - 56s 356ms/step - loss: 0.6717 - acc: 0.5965 - val_loss: 0.6912 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "158/158 [==============================] - 59s 373ms/step - loss: 0.6617 - acc: 0.6062 - val_loss: 0.6921 - val_acc: 0.5020\n",
      "Epoch 3/10\n",
      "158/158 [==============================] - 82s 518ms/step - loss: 0.6622 - acc: 0.6141 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "Epoch 4/10\n",
      "158/158 [==============================] - 68s 429ms/step - loss: 0.6546 - acc: 0.6185 - val_loss: 0.6882 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "158/158 [==============================] - 99s 629ms/step - loss: 0.6431 - acc: 0.6329 - val_loss: 0.6862 - val_acc: 0.4990\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "Epoch 6/10\n",
      "158/158 [==============================] - 84s 534ms/step - loss: 0.6180 - acc: 0.6614 - val_loss: 0.6819 - val_acc: 0.5010\n",
      "Epoch 7/10\n",
      "158/158 [==============================] - 86s 542ms/step - loss: 0.5780 - acc: 0.6920 - val_loss: 0.6727 - val_acc: 0.5121\n",
      "Epoch 8/10\n",
      "158/158 [==============================] - 88s 555ms/step - loss: 0.5198 - acc: 0.7423 - val_loss: 0.6846 - val_acc: 0.5010\n",
      "Epoch 9/10\n",
      "158/158 [==============================] - 88s 554ms/step - loss: 0.4719 - acc: 0.7729 - val_loss: 0.7035 - val_acc: 0.4960\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "Epoch 10/10\n",
      "158/158 [==============================] - 114s 720ms/step - loss: 0.4156 - acc: 0.8046 - val_loss: 0.7331 - val_acc: 0.4909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fee31064e50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the callback and training the model\n",
    "call_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.95, patience=2, verbose=2,\n",
    "                                mode='auto', min_delta=0.01, cooldown=0, min_lr=0)\n",
    "\n",
    "model.fit_generator(train_generator(), steps_per_epoch=batches_per_epoch, epochs=10,\n",
    "                    validation_data=val_generator(), validation_steps=batches_per_epoch_val, callbacks =[call_reduce] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(): # function to generate batches of corresponding batch size\n",
    "    x_list= x_test0\n",
    "    y_list =  y_test0\n",
    "    # Generate batches\n",
    "    while True:\n",
    "        for b in range(batches_per_epoch_test):\n",
    "            if(b == batches_per_epoch_test-1): # An extra if else statement just to manage the last batch as it's size might not be equal to batch size \n",
    "              longest_index = num_sequences_test - 1\n",
    "              timesteps = len(max(x_test0[:longest_index + 1][-batch_size_test:], key=len))\n",
    "              x_train = np.full((longest_index - b*batch_size_test, timesteps, num_features), -99.)\n",
    "              y_train = np.zeros((longest_index - b*batch_size_test,  1))\n",
    "              for i in range(longest_index - b*batch_size_test):\n",
    "                  li = b * batch_size_test + i\n",
    "                  x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                  y_train[i] = y_list[li]\n",
    "            else:\n",
    "                longest_index = (b + 1) * batch_size_test - 1\n",
    "                timesteps = len(max(x_test0[:(b + 1) * batch_size_test][-batch_size_test:], key=len))\n",
    "                x_train = np.full((batch_size_test, timesteps, num_features), -99.)\n",
    "                y_train = np.zeros((batch_size_test,  1))\n",
    "                # padding the vectors with respect to the maximum sequence of each batch and not the whole test data\n",
    "                for i in range(batch_size_test):\n",
    "                    li = b * batch_size_test + i\n",
    "                    x_train[i, 0:len(x_list[li]), :] = x_list[li]\n",
    "                    y_train[i] = y_list[li]\n",
    "            yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1687067747116089, 0.4617414176464081]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences_test = len(x_test0)\n",
    "batch_size_test = 32\n",
    "batches_per_epoch_test = int(num_sequences_test/batch_size_test) + 1\n",
    "num_features= 768\n",
    "# evaluating on the test data\n",
    "model.evaluate_generator(test_generator(), steps= batches_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function which calculates various metrics such as micro and macro precision, accuracy and f1\n",
    "def metrics_calculator(preds, test_labels):\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    TP = []\n",
    "    FP = []\n",
    "    FN = []\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[i][j]\n",
    "\n",
    "        FN.append(summ)\n",
    "    for i in range(0,2):\n",
    "        summ = 0\n",
    "        for j in range(0,2):\n",
    "            if(i!=j):\n",
    "                summ=summ+cm[j][i]\n",
    "\n",
    "        FP.append(summ)\n",
    "    for i in range(0,2):\n",
    "        TP.append(cm[i][i])\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for i in range(0,2):\n",
    "        precision.append(TP[i]/(TP[i] + FP[i]))\n",
    "        recall.append(TP[i]/(TP[i] + FN[i]))\n",
    "\n",
    "    macro_precision = sum(precision)/2\n",
    "    macro_recall = sum(recall)/2\n",
    "    micro_precision = sum(TP)/(sum(TP) + sum(FP))\n",
    "    micro_recall = sum(TP)/(sum(TP) + sum(FN))\n",
    "    micro_f1 = (2*micro_precision*micro_recall)/(micro_precision + micro_recall)\n",
    "    macro_f1 = (2*macro_precision*macro_recall)/(macro_precision + macro_recall)\n",
    "    return macro_precision, macro_recall, macro_f1, micro_precision, micro_recall, micro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4597818734421186, 0.46222902942277067, 0.46100220387193586, 0.46174142480211083, 0.46174142480211083, 0.46174142480211083)\n"
     ]
    }
   ],
   "source": [
    "# getting the predicted labels on the test data\n",
    "preds = model.predict_generator(test_generator(), steps= batches_per_epoch_test)\n",
    "y_pred = preds > 0.5\n",
    "\n",
    "# Calculating all metrics on test data predicted label\n",
    "print(metrics_calculator(y_pred, y_test0[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.44530350185017276, 0.49092741935483875, 0.46700380011291553, 0.4909274193548387, 0.4909274193548387, 0.4909274193548387)\n"
     ]
    }
   ],
   "source": [
    "# getting the predicted labels on the dev data\n",
    "preds = model.predict_generator(val_generator(), steps= batches_per_epoch_val)\n",
    "y_pred_dev = preds > 0.5\n",
    "\n",
    "# Calculating all metrics on dev data predicted label\n",
    "print(metrics_calculator(y_pred_dev, y_dev0[:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
